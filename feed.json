{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hexbee&#x27;s Dev Journal",
    "description": "",
    "home_page_url": "https://xav.github.io",
    "feed_url": "https://xav.github.io/feed.json",
    "user_comment": "",
    "icon": "https://xav.github.io/media/website/Logo-SideName.png",
    "author": {
        "name": "Xavier Basty Kjellberg"
    },
    "items": [
        {
            "id": "https://xav.github.io/11-understanding-the-graphics-pipeline.html",
            "url": "https://xav.github.io/11-understanding-the-graphics-pipeline.html",
            "title": "1.1 - Understanding the Graphics Pipeline",
            "summary": "What We’re Learning Welcome to the start of your shader programming journey! Before we can write a single line of WGSL code to create shimmering water or a glowing sword, we must first answer a fundamental question: what exactly is a shader? At its core, a shader is&hellip;",
            "content_html": "<h2 id=\"what-were-learning\">What We’re Learning</h2>\n<p>Welcome to the start of your shader programming journey! Before we can write a single line of WGSL code to create shimmering water or a glowing sword, we must first answer a fundamental question: <strong>what exactly is a shader?</strong></p><p>At its core, a shader is a small, highly-focused program that you, the developer, write. Unlike the Rust code that runs on your computer’s main processor (the CPU), a shader runs directly on the thousands of parallel cores of your Graphics Processing Unit (GPU). This gives you direct, low-level control over how your game’s graphics are rendered. Shaders are the modern key to controlling the look, feel, and performance of everything you see in a real-time 3D application.</p><p>This first chapter pulls back the curtain on the rendering process. We will build a complete mental model of the <strong>graphics pipeline</strong> - the step-by-step assembly line a GPU uses to turn the raw data of your 3D models into the final, vibrant pixels on your screen.</p><p>Understanding this pipeline is the single most important foundation for shader programming. Without it, writing shader code is like trying to assemble a car without knowing what an engine or a wheel does. With it, every new concept will have a clear place to belong.</p><p>By the end of this chapter, you’ll understand:</p><ul>\n<li><strong>The Big Picture:</strong> The journey of your 3D model data from the CPU (your Bevy app) to the GPU (the rendering factory).</li>\n<li><strong>The “Why” of GPUs:</strong> Why we need a specialized processor for graphics and how its parallel design is perfect for rendering.</li>\n<li><strong>The Vertex’s Journey:</strong> How a single point in your 3D model travels through a series of “coordinate spaces” - from its local origin to its final position on your screen.</li>\n<li><strong>The Two Programmable Stages:</strong> The specific jobs of the <strong>Vertex Shader</strong> (controlling shape and position) and the <strong>Fragment Shader</strong> (controlling color and appearance), and how your WGSL code fits into this process.</li>\n</ul>\n<h2 id=\"the-big-picture-cpu-to-screen\">The Big Picture: CPU to Screen</h2>\n<p>Let’s start with the fundamental question: <strong>How does a 3D model in your Bevy game become the final, colored pixels on your screen?</strong></p><p>The process is a hand-off of instructions and data from a general-purpose processor (the CPU) to a highly specialized one (the GPU). The CPU decides <em>what</em> needs to be drawn, while the GPU figures out how to draw it at incredible speed. The GPU performs this task using a dedicated assembly line called the <strong>graphics pipeline</strong>.</p><p>Here is a simplified map of that journey:</p><pre><code class=\"language-plaintext\">┌───────────────────────────────────────────┐\n│              CPU: The Director            │\n│          (Your Bevy / Rust Code)          │\n│                                           │\n│  - Runs game logic, physics, AI, etc.     │\n│  - Decides WHAT to draw this frame.       │\n│  - Packages all necessary data for GPU.   │\n└───────────────────────────────────────────┘\n                       │\n                       │ Sends a &quot;Draw Command&quot; with all required data\n                       ↓\n  ═════════════════════ GPU BOUNDARY ═════════════════════\n                       ↓\n┌───────────────────────────────────────────┐\n│             GPU: The Factory              │\n│       (Massively Parallel Hardware)       │\n└───────────────────────────────────────────┘\n                       │\n                       ▼\n┌───────────────────────────────────────────┐\n│            1. VERTEX SHADER               │ «── [ YOUR WGSL CODE RUNS HERE ]\n├───────────────────────────────────────────┤\n│ INPUT:  A single vertex from a mesh       │\n│         (e.g., its local position, UVs).  │\n│                                           │\n│ JOB:    Calculate the vertex&#39;s final      │\n│         position on the screen.           │\n│                                           │\n│ OUTPUT: The vertex&#39;s position in          │\n│         &quot;Clip Space&quot; &amp; other data         │\n│         (like normals) for the next stage.│\n└───────────────────────────────────────────┘\n                       │\n                       │ (GPU groups 3 processed vertices into a triangle)\n                       ↓\n┌───────────────────────────────────────────┐\n│            2. RASTERIZATION               │ «── [ AUTOMATIC HARDWARE STAGE ]\n├───────────────────────────────────────────┤\n│ INPUT:  A triangle in screen space.       │\n│                                           │\n│ JOB:    Determine which pixels the        │\n│         triangle covers.                  │\n│                                           │\n│ OUTPUT: A stream of &quot;Fragments.&quot;          │\n│         (A fragment is a potential pixel  │\n│         with smoothly interpolated data). │\n└───────────────────────────────────────────┘\n                       │\n                       ↓\n┌───────────────────────────────────────────┐\n│           3. FRAGMENT SHADER              │ «── [ YOUR WGSL CODE RUNS HERE ]\n├───────────────────────────────────────────┤\n│ INPUT:  A single fragment with its        │\n│         interpolated data (e.g., UVs).    │\n│                                           │\n│ JOB:    Calculate the final color for     │\n│         that specific fragment.           │\n│                                           │\n│ OUTPUT: A single RGBA color.              │\n└───────────────────────────────────────────┘\n                       │\n                       ↓\n┌───────────────────────────────────────────┐\n│           4. OUTPUT MERGER                │ «── [ AUTOMATIC HARDWARE STAGE ]\n├───────────────────────────────────────────┤\n│ JOB:    Take the colored fragment and     │\n│         merge it into the final image.    │\n│                                           │\n│       - Performs Depth Test (is this      │\n│         fragment behind something else?). │\n│       - Performs Blending (for            │\n│         transparency effects).            │\n│       - Writes the final color.           │\n└───────────────────────────────────────────┘\n                       │\n                       ▼\n            ▓▓▓ PIXELS ON SCREEN ▓▓▓\n</code></pre>\n<p>This process spans two distinct worlds, each with a specific job.</p><h3 id=\"the-cpu-side-bevyrust-the-director\">The CPU Side (Bevy/Rust): The Director</h3>\n<p>Think of your CPU and your Bevy code as the <strong>director of a film</strong>. It’s smart, flexible, and responsible for all the high-level decision-making. For every single frame (60 times a second!), it runs your game logic, updates physics, handles input, and determines what needs to be on screen and where it should be.</p><p>Its final job for rendering is to prepare a detailed set of instructions and data for the GPU. This “frame package” includes:</p><ul>\n<li><strong>Mesh Data</strong>: The raw vertex information (positions, normals, UV coordinates) for each model.</li>\n<li><strong>Material Data</strong>: Your shader’s settings, like colors, roughness values, and which textures to use.</li>\n<li><strong>Transformation Data</strong>: The matrices that define each object’s position, rotation, and scale in the world.</li>\n<li><strong>Global Data</strong>: Information about the entire scene, like the camera’s position (View Matrix), its lens settings (Projection Matrix), and the location of lights.</li>\n</ul>\n<p>Once this package is assembled, the CPU sends it across the bus to the GPU and effectively says, “Here, render this.” The CPU’s main rendering job for this frame is now complete.</p><h3 id=\"the-gpu-side-shaders-the-vfx-studio\">The GPU Side (Shaders): The VFX Studio</h3>\n<p>Think of the GPU as a state-of-the-art <strong>VFX studio</strong> with an army of digital artists, each with a very specific job. This studio is built for one purpose: turning the director’s brief into a final, beautifully rendered image with incredible speed. Their process is the graphics pipeline.</p><p>When the brief arrives at the studio:</p><ol>\n<li><strong>The Geometry &amp; Layout Artists (Vertex Shader):</strong> The first team gets the brief. This team consists of thousands of artists. Each artist is assigned a single corner (a <strong>vertex</strong>) of an actor or prop. Their only job is to calculate exactly where that specific corner will appear in the final camera shot, based on the actor’s position and the camera’s lens. They don’t color anything; they just map out the structure of the scene from the camera’s perspective.</li>\n<li><strong>The Rendering Artists (Fragment Shader):</strong> After the layout is done, a second, even larger army of artists takes over. There are millions of them - one for every pixel of the final image. Each artist is assigned a single pixel to paint (a <strong>fragment</strong>). They look at the director’s notes for that surface (the material, the textures) and the lighting setup. Then, they calculate and apply the final, precise color for their one tiny pixel.</li>\n</ol>\n<p>This analogy directly maps to the GPU’s strengths:</p><ul>\n<li><strong>Specialization:</strong> The geometry artists only do positioning; the rendering artists only do coloring.</li>\n<li><strong>Parallelism:</strong> Millions of rendering artists can paint their individual pixels all at the exact same time, without needing to talk to each other. This is what makes the GPU so fast.</li>\n</ul>\n<p>Your WGSL shader code is the set of instructions - the “artistic direction” - you give to these two teams of digital artists.</p><h2 id=\"why-the-gpu-understanding-parallelism\">Why the GPU? Understanding Parallelism</h2>\n<p>It’s a fair question: your computer’s Central Processing Unit (CPU) is an incredibly powerful and fast processor. Why can’t it just draw the triangles? Why do we need a separate, specialized piece of hardware like a Graphics Processing Unit (GPU)?</p><p>The answer lies not in raw clock speed, but in a fundamentally different architectural philosophy: <strong>Serial vs. Parallel processing</strong>.</p><h3 id=\"the-cpu-a-master-chef\">The CPU: A Master Chef</h3>\n<p>Think of your CPU as a <strong>master chef in a world-class kitchen</strong>.</p><ul>\n<li><strong>It is brilliant and versatile.</strong> It can follow any recipe (run any program), handle complex sequential steps, improvise when needed (handle interrupts and varied tasks), and manage the entire kitchen (the operating system).</li>\n<li><strong>It has a few, very powerful cores.</strong> Like having 8 or 16 highly trained sous-chefs, it can work on a handful of complex, different dishes at once.</li>\n</ul>\n<p>If you asked this master chef to prepare a single, elaborate seven-course meal, they would excel. But if you asked them to make ten million identical hamburgers, the entire restaurant would grind to a halt. The chef’s genius is wasted on such a simple, repetitive task; its strength is in complexity and flexibility, not mass production.</p><h3 id=\"the-gpu-a-hyper-efficient-assembly-line\">The GPU: A Hyper-Efficient Assembly Line</h3>\n<p>Now, think of your GPU as a <strong>massive hamburger assembly line that stretches for miles</strong>.</p><ul>\n<li><strong>It is highly specialized.</strong> It’s not designed to create new recipes. It’s designed to execute one simple recipe over and over again with breathtaking speed.</li>\n<li><strong>It has thousands of simple cores.</strong> Instead of a few master chefs, you have thousands of line cooks. Each cook is trained for just one or two simple tasks - place the patty, add the cheese - but they all work at the exact same time.</li>\n</ul>\n<p>This factory can’t prepare a seven-course meal, but it can produce those ten million identical hamburgers in the blink of an eye. This is what we call <strong>“pleasingly parallel”</strong> work.</p><h3 id=\"graphics-is-a-pleasingly-parallel-problem\">Graphics is a “Pleasingly Parallel” Problem</h3>\n<p>Rendering a 3D scene is the ultimate assembly-line task. The core operations are simple, repetitive, and most importantly, <strong>independent</strong>.</p><ul>\n<li>The calculation for vertex A’s final position does not depend on vertex B’s position.</li>\n<li>The calculation for pixel #1’s color does not depend on pixel #2’s color.</li>\n</ul>\n<p>They can all be processed simultaneously.</p><p>Let’s put this into perspective. When rendering a moderately complex scene with 100,000 triangles on a standard 1080p display, for a single frame, you are asking the hardware to perform approximately:</p><ul>\n<li><strong>300,000 vertex shader executions</strong> (one for each vertex)</li>\n<li><strong>Millions of fragment shader executions</strong> (one for each pixel covered by a triangle)</li>\n</ul>\n<p>And this has to happen <strong>60 times every second</strong> for smooth gameplay.</p><p>This is the “ten million hamburgers” problem. A CPU, with its few brilliant cores, would be overwhelmed trying to handle these tasks one by one. But a GPU, with its thousands of simple cores, can process huge batches of vertices and pixels all at once.</p><p>This is why we write shaders for the GPU. We are providing the “recipe” for the assembly line workers. The GPU’s architecture is not just “more cores”; it’s a completely different philosophy of computation, one that is perfectly and beautifully matched to the massive, repetitive, and parallel nature of computer graphics.</p><h2 id=\"the-rendering-pipeline-in-detail\">The Rendering Pipeline in Detail</h2>\n<p>Let’s zoom in on the VFX studio’s assembly line. Each stage has a specific responsibility, taking a particular kind of data as input and producing a new kind of data for the next stage to work on.</p><h3 id=\"stage-1-the-application-stage-cpu---the-directors-brief\">Stage 1: The Application Stage (CPU - The Director’s Brief)</h3>\n<p>This stage isn’t on the GPU at all - it’s your Bevy application running on the CPU. Think of it as the “pre-production” step where the director prepares the detailed brief. Before any rendering can happen, your application needs to tell the GPU everything it needs to know about the world for the upcoming frame. Bevy’s renderer orchestrates this for you.</p><p>For every frame, Bevy traverses your scene’s Entity-Component-System (ECS) world and gathers all the necessary information:</p><pre><code class=\"language-rust\">// You write this in Bevy, describing the &quot;what&quot; and &quot;where&quot;\ncommands.spawn((\n Mesh3d(meshes.add(Sphere::new(1.0))),\n MeshMaterial3d(materials.add(my_custom_material)),\n));\n</code></pre>\n<p>From code like this, Bevy assembles the “shot list”:</p><ul>\n<li><strong>What to draw</strong>: A list of meshes (the sphere’s vertices and triangles).</li>\n<li><strong>How to draw it</strong>: The material to use and its properties (colors, textures).</li>\n<li><strong>Where it is</strong>: The object’s world position, rotation, and scale (the Transform).</li>\n<li><strong>From where to view it</strong>: The camera’s position and perspective settings.</li>\n</ul>\n<p>The final output of this stage is a highly-organized package of data and commands, which Bevy then sends over to the GPU to begin the actual rendering process.</p><h3 id=\"stage-2-the-vertex-shader-gpu---the-layout-artists\">Stage 2: The Vertex Shader (GPU - The Layout Artists)</h3>\n<p>This is the <strong>first programmable stage</strong> on the GPU, where your first piece of WGSL code runs. The vertex shader’s fundamental job is to answer one question for every single vertex of a mesh: <strong>“Where on the screen does this vertex end up?”</strong></p><ul>\n<li><strong>Input</strong>: It receives the data for a <strong>single vertex</strong> at a time (its position in local model space, its normal vector, its UV coordinates, etc.).</li>\n<li><strong>The Job</strong>: Its one mandatory task is to perform mathematical operations (usually matrix multiplications) to transform the vertex’s 3D position into a final 4D “clip space” position. This clip space coordinate is what the GPU hardware needs to figure out the 2D location on your monitor.</li>\n<li><strong>Output</strong>: It must output that final clip space position. It can also pass along any other data it received or calculated (like colors, normals, or UVs) to be used later by the fragment shader.</li>\n</ul>\n<p>Here’s a conceptual view of what your WGSL code will do:</p><pre><code class=\"language-wgsl\">@vertex\nfn vertex(input: VertexInput) -&gt; VertexOutput {\n    var output: VertexOutput;\n\n    // The primary job: Transform the 3D local position into a final 2D screen position.\n    output.position = project_to_screen(view_matrix * model_matrix * input.position);\n\n    // A secondary job: Pass necessary data to the next stage.\n    // Here, we&#39;re just passing the vertex&#39;s normal along for lighting calculations later.\n    output.normal = input.normal;\n    return output;\n}\n</code></pre>\n<p><strong>Key Insight</strong>: Because this shader runs on every vertex, it gives you the power to manipulate the shape and position of your geometry in real-time. This is not just about moving objects around (which is best done by changing the Transform in Bevy); it’s about deforming the mesh itself. This is how you create dynamic effects like:</p><ul>\n<li>Waving flags</li>\n<li>Rippling water surfaces</li>\n<li>Procedurally animated grass swaying in the wind</li>\n</ul>\n<h3 id=\"stage-3-the-rasterizer-gpu---automatic-hardware\">Stage 3: The Rasterizer (GPU - Automatic Hardware)</h3>\n<p>This stage is a piece of dedicated, non-programmable hardware on the GPU. It’s an automatic, incredibly fast process that you don’t write code for. The rasterizer takes the processed vertices from the vertex shader (three at a time to form a triangle) and figures out which pixels on the screen that triangle covers.</p><p>For every single pixel it covers, it generates a <strong>“fragment.”</strong> A fragment is a “potential pixel” - it contains all the information needed to calculate a final color.</p><p>This is also where the magic of <strong>interpolation</strong> happens. The rasterizer looks at the data you passed out of the vertex shader for each of the triangle’s three vertices and smoothly blends it across the surface of the triangle for each fragment.</p><p>Think of it like this:</p><p>![[figure - Rasterizer.png]]</p><ul>\n<li>A fragment near the top will receive a reddish color.</li>\n<li>A fragment on the left edge will get a purplish color (red + blue).</li>\n<li>A fragment right in the middle will get a muddy, grayish color, which is the mathematical average of red, green, and blue.</li>\n</ul>\n<p>The rasterizer does this for every single piece of data you passed along - colors, UV coordinates, normals, etc. The output is a massive stream of fragments, each one “pre-loaded” with its own unique, interpolated data, ready to be colored.</p><h3 id=\"stage-4-the-fragment-shader-gpu---the-rendering-artists\">Stage 4: The Fragment Shader (GPU - The Rendering Artists)</h3>\n<p>This is the <strong>second programmable stage</strong>, and it’s where most of the visual artistry happens. The fragment shader’s job is to answer one simple question for every single fragment generated by the rasterizer: <strong>“What color is this pixel?”</strong></p><ul>\n<li><strong>Input</strong>: It receives the interpolated data for a <strong>single fragment</strong>. This includes its position on the screen and the smoothly blended values (like normals and UVs) that were passed from the vertex shader.</li>\n<li><strong>The Job</strong>: To use this input data to perform calculations and return a single, final color.</li>\n<li><strong>Output</strong>: It must output a <code>vec4&lt;f33&gt;</code> representing an RGBA color. This is the color that will be written to the screen (assuming it passes final tests like depth testing).</li>\n</ul>\n<p>Here’s a conceptual view of what your WGSL code will do:</p><pre><code class=\"language-wgsl\">@fragment\nfn fragment(input: FragmentInput) -&gt; @location(0) vec4&lt;f32&gt; {\n    // The input data (e.g., input.normal) has already been smoothly interpolated for us.\n    // We can use it to calculate lighting for this specific pixel.\n    let lighting = calculate_light(input.normal, light_direction);\n\n    // We could also sample a texture using the interpolated UVs.\n    let surface_color = sample_texture(input.uv);\n\n    // Combine them to get the final color.\n    let final_color = surface_color * lighting;\n    return vec4&lt;f32&gt;(final_color, 1.0); // Return the final RGBA value\n}\n</code></pre>\n<p><strong>Key Insight</strong>: The fragment shader runs for potentially millions of pixels every frame. This is your chance to define the appearance of your object’s surface with incredible detail. This is where you:</p><ul>\n<li>Apply lighting calculations to create highlights and shadows.</li>\n<li>Sample textures to give a surface detail and color.</li>\n<li>Create procedural patterns like stripes, checkerboards, or noise.</li>\n<li>Implement special effects like glowing, outlines, or distortion.</li>\n</ul>\n<h2 id=\"the-coordinate-space-journey\">The Coordinate Space Journey</h2>\n<p>One of the most initially confusing, but ultimately powerful, concepts in graphics programming is the journey a vertex takes through different <strong>coordinate spaces</strong>. Each space is a different frame of reference, like describing a location from a different point of view. The vertex shader’s main job is to transform a vertex from one space to the next, like a series of conversions, until it lands in the right spot on the screen.</p><p>Think of it like giving directions to a friend:</p><ol>\n<li>“The book is on the second shelf of the bookcase.” (<strong>Local Space</strong>: The position is relative to the bookcase).</li>\n<li>“The bookcase is against the north wall of the living room.” (<strong>World Space</strong>: Now the bookcase’s position is relative to the entire house).</li>\n<li>“Stand in the doorway and look towards the fireplace.” (<strong>View Space</strong>: Now everything is described from your friend’s point of view).</li>\n<li>“The book you’re looking for should be in the upper-left part of your vision.” (<strong>Clip Space</strong>: This is what’s in their field of view).</li>\n</ol>\n<p>In 3D graphics, we perform these transformations using matrix mathematics.</p><p><strong>Note</strong>: Don’t worry if the matrix math looks intimidating right now. We have a dedicated chapter later that explains how matrices work. For now, just focus on the purpose of each transformation - what question it answers.</p><h3 id=\"1-local-space-or-model-space\">1. Local Space (or Model Space)</h3>\n<p><strong>The Question:</strong> “What does this object look like by itself?”</p><p>This is the object’s blueprint. When an artist creates a 3D model of a car, they don’t care where it will be in your game world. They model it at the center of its own universe, the origin <code>(0, 0, 0)</code>. The coordinates of every vertex are relative only to the car’s own center point.</p><pre><code class=\"language-plaintext\">A car&#39;s front-right tire vertex in Local Space might be:\n(0.8, 0.3, 1.5)\n</code></pre>\n<h3 id=\"2-world-space\">2. World Space</h3>\n<p><strong>The Question:</strong> “Where is this object in the game world?”</p><p>This is the shared, global coordinate system of your entire scene. It’s the common frame of reference where all your objects, lights, and the camera coexist. When you give an entity a Transform in Bevy, you are defining its position, rotation, and scale within this World Space.</p><pre><code class=\"language-rust\">// This Transform moves the object from its local origin to a specific spot in the world.\nTransform::from_xyz(10.0, 0.0, -20.0)\n</code></pre>\n<p>The <strong>Model Matrix</strong>, which Bevy derives from this <code>Transform</code>, is the mathematical tool that converts vertices from <strong>Local Space</strong> to <strong>World Space</strong>. Your vertex shader applies it to every vertex.</p><pre><code class=\"language-plaintext\">Local Position × Model Matrix = World Position\n</code></pre>\n<h3 id=\"3-view-space-or-camera-space\">3. View Space (or Camera Space)</h3>\n<p><strong>The Question:</strong> “How does the world look from the camera’s perspective?”</p><p>Once everything is placed in the world, we need to view it. To make the math simpler for the next step, we transform the entire world so that the camera is at the origin <code>(0, 0, 0)</code> and looking down a specific axis (typically negative Z). Everything in the world is now positioned relative to the camera. An object in front of the camera will have a negative Z coordinate, an object to the camera’s left will have a negative X, and so on.</p><p>The <strong>View Matrix</strong> performs this transformation. It’s calculated from the camera’s own world-space transform.</p><pre><code class=\"language-plaintext\">World Position × View Matrix = View Position\n</code></pre>\n<h3 id=\"4-clip-space\">4. Clip Space</h3>\n<p><strong>The Question:</strong> “Is this vertex inside the viewable area, and if so, where?”</p><p>This is the final and most abstract space that the vertex shader is responsible for creating. It’s a standardized, cube-like volume that represents everything the camera can see. The transformation into this space, performed by the <strong>Projection Matrix</strong>, does two magical things:</p><ol>\n<li><strong>It applies perspective.</strong> It mathematically squishes the 3D scene so that objects farther away from the camera appear smaller than objects that are closer. This is what creates the illusion of depth.</li>\n<li><strong>It normalizes the coordinates.</strong> Everything that will be visible on screen is mapped into a neat box where the X and Y coordinates range from -1 to 1. The Z coordinate is also remapped (usually to a <code>[0, 1]</code> range) to represent depth.</li>\n</ol>\n<p>The GPU now has a simple job: any vertex with X or Y coordinates outside of this <code>[-1, 1]</code> range is “clipped” and discarded, as it is off-screen. The Z coordinate will be used in a later stage for depth testing (figuring out if one object is in front of another).</p><pre><code class=\"language-plaintext\">View Position × Projection Matrix = Clip Position\n</code></pre>\n<p>The mandatory output of your vertex shader is this final Clip Space position.</p><h3 id=\"5-screen-space\">5. Screen Space</h3>\n<p><strong>The Question:</strong> “Which specific pixel on my monitor does this correspond to?”</p><p>This final step is handled <strong>automatically by the GPU</strong> after the vertex shader is done. The hardware takes the <code>[-1, 1]</code> Clip Space coordinates and maps them to the actual pixel coordinates of your window (e.g., from <code>(0, 0)</code> in the top-left to <code>(1920, 1080)</code> in the bottom-right). This is not something you calculate in your shaders; it’s the final output of the fixed-function part of the pipeline before the fragment shader runs.</p><h2 id=\"putting-it-all-together-the-life-of-a-single-vertex\">Putting It All Together: The Life of a Single Vertex</h2>\n<p>Let’s trace the complete life of a single vertex, from its creation in a modeling tool to its final appearance as a colored pixel on your screen. We’ll follow a vertex at the very top of a sphere model.</p><h3 id=\"1-the-blueprint-cpu---local-space\">1. The Blueprint (CPU - Local Space)</h3>\n<p>It begins its life in a 3D modeling program. An artist defines a sphere, and our vertex is created at the very top. Relative to the sphere’s own center, its position is simply (0.0, 1.0, 0.0). This is its <strong>Local Space</strong> position. This data is loaded into Bevy as part of a Mesh asset.</p><h3 id=\"2-setting-the-scene-cpu---the-directors-brief\">2. Setting the Scene (CPU - The Director’s Brief)</h3>\n<p>Your Bevy application decides where this sphere belongs in the game world. You assign it a <code>Transform</code> to place it, for example, at world coordinates <code>(5.0, 2.0, -3.0)</code>. The CPU doesn’t move the vertex itself; instead, it calculates a <strong>Model Matrix</strong> from this transform and packages it up, ready to be sent to the GPU’s VFX studio.</p><h3 id=\"3-the-great-transformation-gpu---vertex-shader\">3. The Great Transformation (GPU - Vertex Shader)</h3>\n<p>Now the director’s brief is uploaded to the GPU, and our programmable <strong>Vertex Shader</strong> takes over. This is where the “Layout Artists” get to work. The shader receives our vertex’s original local position, <code>(0, 1, 0)</code>, along with the Model, View, and Projection matrices. It then performs the crucial sequence of multiplications to find the vertex’s final on-screen position:</p><ul>\n<li><strong>To World Space</strong>: The shader multiplies the local position by the Model Matrix. This moves the vertex into the shared <strong>World Space</strong>. Our vertex at <code>(0, 1, 0)</code> is now effectively at <code>(5.0, 3.0, -3.0)</code> in the game world (position + model’s height).</li>\n<li><strong>To View Space</strong>: Next, it multiplies the new world position by the View Matrix. This transforms the vertex into <strong>View Space</strong>, making its coordinates relative to the camera’s perspective. Its position might now be something like <code>(-2.0, 1.0, -5.0)</code>, meaning it’s slightly to the camera’s left, above its center, and some distance in front of it.</li>\n<li><strong>To Clip Space</strong>: Finally, it multiplies the view position by the Projection Matrix. This applies perspective and transforms the vertex into the final, required <strong>Clip Space</strong>. The position might now be <code>(-0.4, 0.2, 0.8)</code>. This tells the GPU the vertex is on-screen (since X and Y are between <code>-1</code> and <code>1</code>) and provides its depth. This <code>vec4</code> is the mandatory output of the vertex shader.</li>\n</ul>\n<p>Simultaneously, the vertex shader also prepares any other data needed for coloring, like the vertex’s color or normal vector, and passes it along.</p><h3 id=\"4-the-triangle-factory-gpu---rasterizer\">4. The Triangle Factory (GPU - Rasterizer)</h3>\n<p>The vertex, now just a point in Clip Space, is grouped by the GPU with two other processed vertices to form a triangle. The hardware <strong>Rasterizer</strong> takes over, calculating exactly which screen pixels this triangle covers. For each covered pixel, it generates a “fragment” and <strong>interpolates</strong> all the data that the vertex shaders passed out (like colors or UVs), creating a smooth gradient of values across the triangle’s face.</p><h3 id=\"5-the-coloring-book-gpu---fragment-shader\">5. The Coloring Book (GPU - Fragment Shader)</h3>\n<p>A single fragment, born from the rasterizer, arrives at our programmable <strong>Fragment Shader</strong>. Now the “Rendering Artists” do their job. The fragment carries its own unique, interpolated data (e.g., a color that is a blend of the three corner vertices’ colors). The fragment shader’s sole job is to use this information to calculate a final RGBA color. It might sample a texture, calculate lighting, or perform any number of other operations.</p><h3 id=\"6-the-final-pixel-gpu---output-merger\">6. The Final Pixel (GPU - Output Merger)</h3>\n<p>The fragment shader outputs its calculated color. This color, along with the fragment’s depth, is sent to the final hardware stage. The GPU performs a <strong>depth test</strong> to see if this fragment is in front of whatever is already on the screen at that pixel. If it is, its color is written to the framebuffer, becoming one of the millions of pixels that form the final image you see. Our vertex has completed its journey.</p><h2 id=\"a-mental-model-for-your-shaders\">A Mental Model for Your Shaders</h2>\n<p>The rendering pipeline can seem complex, but by holding on to our “Director and VFX Studio” analogy, we can assign a clear, relatable job to each programmable stage.</p><h3 id=\"1-the-vertex-shader-the-vfx-layout-artist\">1. The Vertex Shader: The VFX Layout Artist</h3>\n<p>Your WGSL code in the vertex shader provides the instructions for the layout artists at the VFX studio.</p><table>\n<thead>\n<tr>\n<th>Analogy</th>\n<th>Role</th>\n<th>Shader Code Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Staging the scene for the camera.</strong></td>\n<td>Your job is to take the 3D models (defined in their own local space) and <strong>place them correctly in the world and frame them perfectly in the camera’s shot.</strong></td>\n<td><strong>Transformation.</strong> You use matrix math to move each vertex of a model from its origin to its final position relative to the camera’s view. You can also dynamically move vertices here to create animations like waving flags or rippling water.</td>\n</tr>\n<tr>\n<td><strong>Defining the final composition.</strong></td>\n<td>You must output the final projected position of each vertex. This is the main deliverable for this stage.</td>\n<td><code>output.position = projection * view * model * local_pos;</code></td>\n</tr>\n<tr>\n<td><strong>Prepping for the colorists.</strong></td>\n<td>You pass along any surface information that the next team will need, like texture coordinates (uv) or which way the surface is facing (normal).</td>\n<td><code>output.world_normal = ..., output.uv = ...</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"2-the-rasterizer-the-digital-render-farm\">2. The Rasterizer: The Digital Render Farm</h3>\n<p>This is an automated, non-programmable hardware stage. You don’t write code for it, but you need to know what it does.</p><table>\n<thead>\n<tr>\n<th>Analogy</th>\n<th>Role</th>\n<th>Shader Code Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Automated rendering setup.</strong></td>\n<td>The GPU’s hardware takes the 3D triangles you’ve positioned in the previous step…</td>\n<td><strong>None.</strong> This is a fixed, non-programmable step.</td>\n</tr>\n<tr>\n<td><strong>…and projects them onto a 2D grid.</strong></td>\n<td>The hardware determines exactly which pixels on the screen are covered by each triangle. For each covered pixel, it creates a “fragment” and <strong>interpolates</strong> (smoothly blends) the data from the triangle’s corners.</td>\n<td>A fragment in the middle of a triangle gets a perfectly blended uv coordinate and normal vector, ready for the next stage.</td>\n</tr>\n</tbody></table>\n<h3 id=\"3-the-fragment-shader-the-vfx-coloring--lighting-artist\">3. The Fragment Shader: The VFX Coloring &amp; Lighting Artist</h3>\n<p>Your WGSL code in the fragment shader provides the instructions for the massive team of coloring and lighting artists.</p><table>\n<thead>\n<tr>\n<th>Analogy</th>\n<th>Role</th>\n<th>Shader Code Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Painting one pixel at a time.</strong></td>\n<td>Your job is to look at a single, uncolored pixel (a fragment) and <strong>decide what its final color should be.</strong> This is where all the visual artistry happens.</td>\n<td><strong>Color Calculation.</strong> You have complete control over the pixel’s final RGBA value.</td>\n</tr>\n<tr>\n<td><strong>Using the prepped materials.</strong></td>\n<td>You use the interpolated data from the previous stage to perform your work.</td>\n<td><code>return vec4&lt;f32&gt;(final_color, alpha);</code></td>\n</tr>\n<tr>\n<td><strong>Applying textures and lighting.</strong></td>\n<td>You can sample <strong>textures</strong> using the interpolated uv coordinates and use the interpolated normal vector to calculate realistic <strong>lighting</strong> and shadows.</td>\n<td><code>textureSample(...)</code>, <code>dot(normal, light_dir)</code></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"complete-example-visualizing-the-pipeline\">Complete Example: Visualizing the Pipeline</h2>\n<p>Theory is essential, but seeing is believing. To make these abstract concepts concrete, we’ll build a simple, interactive shader in Bevy. This shader won’t create a realistic object; instead, it will act as a diagnostic tool, allowing us to “see” the data at different stages of the pipeline.</p><p><strong>A Note Before We Begin:</strong> You will see new WGSL syntax (@location, @group, etc.) and Bevy patterns (AsBindGroup, MaterialPlugin) in the code below. <strong>Do not worry about understanding every line right now.</strong> We will break down all of these concepts in detail in the upcoming articles.</p><p>The goal of this example is to <strong>observe the visual output</strong> of each mode and connect it back to the high-level pipeline concepts we just learned:</p><ul>\n<li>How data is <strong>interpolated</strong> across a surface (the smooth normal colors).</li>\n<li>How the fragment shader runs for <strong>every pixel</strong> (the sharp checkerboard).</li>\n<li>How data from the <strong>vertex stage</strong> is used in the <strong>fragment stage</strong> (the height gradient).</li>\n</ul>\n<p>Focus on the “what you’re seeing” part, and treat the code as a preview of what you’ll soon master.</p><h3 id=\"our-goal\">Our Goal</h3>\n<p>We will create a custom material for a sphere that can cycle through three different visualization modes by pressing a key. Each mode will highlight a different core concept of the rendering pipeline that we’ve just discussed.</p><h3 id=\"what-this-project-demonstrates\">What This Project Demonstrates</h3>\n<ul>\n<li><strong>Data Flow and Interpolation:</strong> How data (like normal vectors) passed from the vertex shader is smoothly interpolated by the rasterizer before reaching the fragment shader.</li>\n<li><strong>Per-Fragment Processing:</strong> Proof that the fragment shader runs independently for every single pixel, allowing it to create complex patterns based on a fragment’s world position.</li>\n<li><strong>Vertex Data in Fragment Shaders:</strong> How to use data prepared by the vertex shader (like the final world position) to drive calculations in the fragment shader.</li>\n</ul>\n<h3 id=\"the-shader-assetsshadersdebug_pipelinewgsl\">The Shader (<code>assets/shaders/debug_pipeline.wgsl</code>)</h3>\n<p>This single WGSL file contains both our vertex and fragment shaders. The key element is the material.mode uniform, a number we can change from our Rust code to switch the logic inside the fragment shader.</p><ul>\n<li>The <strong>vertex shader</strong> is standard: it transforms the vertex position into clip space and also passes the vertex’s world position and world normal along to the fragment stage.</li>\n<li>The <strong>fragment shader</strong> uses an <code>if/else if</code> chain based on material.mode to decide how to color the current pixel.</li>\n</ul>\n<pre><code class=\"language-wgsl\">#import bevy_pbr::mesh_functions\n#import bevy_pbr::view_transformations::position_world_to_clip\n#import bevy_pbr::forward_io::VertexOutput\n\nstruct DebugMaterial {\n    mode: u32,\n}\n\n@group(2) @binding(0)\nvar&lt;uniform&gt; material: DebugMaterial;\n\n@vertex\nfn vertex(\n    @builtin(instance_index) instance_index: u32,\n    @location(0) position: vec3&lt;f32&gt;,\n    @location(1) normal: vec3&lt;f32&gt;,\n) -&gt; VertexOutput {\n    var out: VertexOutput;\n    \n    // Get the model transformation matrix\n    let world_from_local = mesh_functions::get_world_from_local(instance_index);\n    \n    // Transform position to world space\n    let world_position = mesh_functions::mesh_position_local_to_world(\n        world_from_local,\n        vec4&lt;f32&gt;(position, 1.0)\n    );\n    \n    // Transform to clip space (final vertex shader output)\n    out.position = position_world_to_clip(world_position.xyz);\n    \n    // Transform normal to world space\n    out.world_normal = mesh_functions::mesh_normal_local_to_world(\n        normal,\n        instance_index\n    );\n    \n    // Pass world position to fragment shader\n    out.world_position = world_position;\n    \n    return out;\n}\n\n@fragment\nfn fragment(in: VertexOutput) -&gt; @location(0) vec4&lt;f32&gt; {\n    // Mode 0: Show normals (demonstrates interpolation between vertices)\n    if material.mode == 0u {\n        // Normals range from -1 to 1, convert to 0-1 for RGB color\n        // Red = X direction, Green = Y direction, Blue = Z direction\n        let color = (in.world_normal + 1.0) * 0.5;\n        return vec4&lt;f32&gt;(color, 1.0);\n    }\n    \n    // Mode 1: Checkerboard pattern (demonstrates per-pixel fragment shader work)\n    if material.mode == 1u {\n        // Create a 3D checkerboard pattern\n        let scale = 3.0;\n        let x = i32(floor(in.world_position.x * scale));\n        let y = i32(floor(in.world_position.y * scale));\n        let z = i32(floor(in.world_position.z * scale));\n        \n        // Use bitwise AND to alternate between 0 and 1\n        let checker = (x + y + z) &amp; 1;\n        \n        if checker == 0 {\n            return vec4&lt;f32&gt;(0.9, 0.9, 0.9, 1.0); // Light gray\n        } else {\n            return vec4&lt;f32&gt;(0.2, 0.2, 0.8, 1.0); // Blue\n        }\n    }\n    \n    // Mode 2: Height-based gradient (demonstrates math in fragment shader)\n    if material.mode == 2u {\n        // Color based on Y position (height) in world space\n        // Map from -2 to 2 range to 0-1 range for color\n        let height = (in.world_position.y + 2.0) / 4.0;\n        let color = vec3&lt;f32&gt;(height, 0.5, 1.0 - height);\n        return vec4&lt;f32&gt;(color, 1.0);\n    }\n    \n    // Default: Solid color\n    return vec4&lt;f32&gt;(1.0, 0.0, 0.0, 1.0);\n}\n</code></pre>\n<h3 id=\"the-rust-material-srcmaterialsdebug_pipeliners\">The Rust Material (<code>src/materials/debug_pipeline.rs</code>)</h3>\n<p>This is the Rust-side definition of our material. It’s a simple struct that mirrors the DebugMaterial struct in our shader, allowing Bevy’s rendering engine to send our chosen mode value to the GPU.</p><pre><code class=\"language-rust\">use bevy::prelude::*;\nuse bevy::render::render_resource::{AsBindGroup, ShaderRef};\n\n#[derive(Asset, TypePath, AsBindGroup, Debug, Clone)]\npub struct DebugPipelineMaterial {\n    #[uniform(0)]\n    pub mode: u32,\n}\n\nimpl Material for DebugPipelineMaterial {\n    fn fragment_shader() -&gt; ShaderRef {\n        &quot;shaders/debug_pipeline.wgsl&quot;.into()\n    }\n\n    fn vertex_shader() -&gt; ShaderRef {\n        &quot;shaders/debug_pipeline.wgsl&quot;.into()\n    }\n}\n</code></pre>\n<p>Don’t forget to add it to <code>src/materials/mod.rs</code>:</p><pre><code class=\"language-rust\">// ... other materials\npub mod debug_pipeline;\n</code></pre>\n<h3 id=\"the-demo-module-srcdemosdebug_pipeliners\">The Demo Module (<code>src/demos/debug_pipeline.rs</code>)</h3>\n<p>This Rust module sets up our Bevy scene. It spawns a sphere with our custom <code>DebugPipelineMaterial</code>, adds a camera and light, and sets up the UI text. Most importantly, it contains the <code>cycle_debug_mode</code> system, which listens for the spacebar press and updates the mode field on our material, triggering the change in the shader.</p><pre><code class=\"language-rust\">use crate::materials::debug_pipeline::DebugPipelineMaterial;\nuse bevy::prelude::*;\n\npub fn run() {\n    App::new()\n        .add_plugins(DefaultPlugins)\n        .add_plugins(MaterialPlugin::&lt;DebugPipelineMaterial&gt;::default())\n        .add_systems(Startup, setup)\n        .add_systems(Update, (rotate_camera, cycle_debug_mode))\n        .run();\n}\n\nfn setup(\n    mut commands: Commands,\n    mut meshes: ResMut&lt;Assets&lt;Mesh&gt;&gt;,\n    mut materials: ResMut&lt;Assets&lt;DebugPipelineMaterial&gt;&gt;,\n) {\n    // Spawn a sphere with our debug material (better for showing interpolation)\n    commands.spawn((\n        Mesh3d(meshes.add(Sphere::new(1.0))),\n        MeshMaterial3d(materials.add(DebugPipelineMaterial { mode: 0 })),\n    ));\n\n    // Light\n    commands.spawn((\n        PointLight {\n            shadows_enabled: true,\n            ..default()\n        },\n        Transform::from_xyz(4.0, 8.0, 4.0),\n    ));\n\n    // Camera\n    commands.spawn((\n        Camera3d::default(),\n        Transform::from_xyz(-2.5, 4.5, 9.0).looking_at(Vec3::ZERO, Vec3::Y),\n    ));\n\n    // UI to show current mode\n    commands.spawn((\n        Text::new(&quot;Press SPACE to cycle debug modes\\nMode 0: Normals (shows interpolation)&quot;),\n        Node {\n            position_type: PositionType::Absolute,\n            top: Val::Px(10.0),\n            left: Val::Px(10.0),\n            ..default()\n        },\n    ));\n}\n\nfn rotate_camera(time: Res&lt;Time&gt;, mut camera_query: Query&lt;&amp;mut Transform, With&lt;Camera3d&gt;&gt;) {\n    for mut transform in camera_query.iter_mut() {\n        let radius = 9.0;\n        let angle = time.elapsed_secs() * 0.5;\n        transform.translation.x = angle.cos() * radius;\n        transform.translation.z = angle.sin() * radius;\n        transform.look_at(Vec3::ZERO, Vec3::Y);\n    }\n}\n\nfn cycle_debug_mode(\n    keyboard: Res&lt;ButtonInput&lt;KeyCode&gt;&gt;,\n    mut materials: ResMut&lt;Assets&lt;DebugPipelineMaterial&gt;&gt;,\n    mut text_query: Query&lt;&amp;mut Text&gt;,\n) {\n    if keyboard.just_pressed(KeyCode::Space) {\n        for (_, material) in materials.iter_mut() {\n            material.mode = (material.mode + 1) % 3;\n\n            for mut text in text_query.iter_mut() {\n                **text = match material.mode {\n                    0 =&gt; &quot;Press SPACE to cycle debug modes\\nMode 0: Normals (shows interpolation)&quot;.to_string(),\n                    1 =&gt; &quot;Press SPACE to cycle debug modes\\nMode 1: Checkerboard (per-pixel processing)&quot;.to_string(),\n                    2 =&gt; &quot;Press SPACE to cycle debug modes\\nMode 2: Height Gradient (fragment math)&quot;.to_string(),\n                    _ =&gt; &quot;Unknown mode&quot;.to_string(),\n                };\n            }\n        }\n    }\n}\n</code></pre>\n<p>Don’t forget to add it to <code>src/demos/mod.rs</code>:</p><pre><code class=\"language-rust\">// ... other demoss\npub mod debug_pipeline;\n</code></pre>\n<p>And register it in <code>src/main.rs</code>:</p><pre><code class=\"language-rust\">Demo {\n    number: &quot;1.1&quot;,\n    title: &quot;Understanding the Graphics Pipeline&quot;,\n    run: demos::debug_pipeline::run,\n},\n</code></pre>\n<h3 id=\"running-the-demo\">Running the Demo</h3>\n<p>When you run the project, you will see a sphere. Pressing the spacebar will cycle through the three different debug visualizations, each revealing a different aspect of the pipeline’s inner workings.</p><h4 id=\"controls\">Controls</h4>\n<table>\n<thead>\n<tr>\n<th>Control</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>SPACE</strong></td>\n<td>Cycle to the next visualization mode (0 -&gt; 1 -&gt; 2 -&gt; 0).</td>\n</tr>\n</tbody></table>\n<h4 id=\"what-youre-seeing\">What You’re Seeing</h4>\n<p>![[screenshot - 1.png]]\n![[screenshot - 2.png]]\n![[screenshot - 3.png]]</p><table>\n<thead>\n<tr>\n<th>Mode</th>\n<th>Description</th>\n<th>What It Proves</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>0 - Normals</strong></td>\n<td>The sphere is colored based on the direction its surface is facing. Red points right (+X), Green points up (+Y), and Blue points forward (+Z). You see smooth gradients of color across the entire surface.</td>\n<td>This demonstrates <strong>interpolation</strong>. The normals are defined only at the vertices, but the rasterizer smoothly blends them between those points, giving every single pixel its own unique normal vector to use for coloring.</td>\n</tr>\n<tr>\n<td><strong>1 - Checkerboard</strong></td>\n<td>The sphere is covered in a 3D checkerboard pattern that appears fixed in the world as the sphere rotates through it. The pattern is sharp and blocky.</td>\n<td>This demonstrates <strong>per-fragment processing</strong>. The fragment shader calculates which color to be (light gray or blue) for every single pixel independently, based on that pixel’s position in world space.</td>\n</tr>\n<tr>\n<td><strong>2 - Height Gradient</strong></td>\n<td>The sphere is colored with a gradient based on its height in the world. The bottom is magenta, and the top is cyan.</td>\n<td>This demonstrates using <strong>vertex shader data in the fragment shader</strong>. The vertex shader calculates the world_position for each vertex. The rasterizer interpolates it, and the fragment shader uses the Y-component of that position to calculate a color.</td>\n</tr>\n</tbody></table>\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<p>This chapter covered a lot of ground. Before moving on, take a moment to solidify these five core concepts. They are the foundation for everything that follows.</p><ol>\n<li><strong>The Pipeline is a CPU-to-GPU Process.</strong><br> Rendering is a collaboration. Your Bevy code on the CPU acts as the <strong>Director</strong>, preparing the scene’s data (meshes, materials, transforms). It then hands this “shot list” to the GPU, a specialized <strong>VFX Studio</strong> that executes the rendering process through a hardware assembly line.</li>\n<li><strong>Shaders are Your Instructions for the VFX Artists.</strong><br> You cannot change the hardware pipeline itself, but you can write small, highly focused programs called <strong>shaders</strong> that run at critical, programmable stages. The GPU’s massively parallel architecture executes your shader code for millions of vertices and pixels per second, which is what makes real-time 3D graphics possible.</li>\n<li><strong>The Vertex Shader’s Job is to POSITION Geometry.</strong><br> This is your “Layout Artist” stage. The vertex shader runs once for every vertex in your mesh. Its primary responsibility is to take that vertex’s 3D position from the original model and transform it through a series of coordinate spaces until it has its final, correct position on the 2D screen.</li>\n<li><strong>The Fragment Shader’s Job is to COLOR Pixels.</strong><br> This is your “Coloring &amp; Lighting Artist” stage. After the hardware rasterizer determines which pixels a triangle covers, the fragment shader runs once for every single one of those “fragments.” Its sole responsibility is to calculate and return the final RGBA color for that specific spot on the screen. This is where you apply textures, lighting, and visual effects.</li>\n<li><strong>Data Flows and is Interpolated from Vertex to Fragment.</strong><br> The two shaders are connected. The vertex shader can pass data (like UV coordinates or normal vectors) to the next stage. The hardware <strong>Rasterizer</strong> automatically <strong>interpolates</strong> (smoothly blends) this data across the face of the triangle, making a unique version of it available to the fragment shader for every single pixel.</li>\n</ol>\n<h2 id=\"whats-next\">What’s Next?</h2>\n<p>You now have the essential mental model for the rendering pipeline - you understand where your shader code runs and the specific job of each stage. With this “map” in hand, we are finally ready to learn the language of the GPU’s artists themselves.</p><p>In the next article, we will dive into the fundamental building blocks of the WGSL language: its data types and variables. You’ll learn how to represent positions, colors, and transformations in your code using scalars, vectors, and matrices.</p><p><em>Next up:</em> <a href=\"https://hexbee.hashnode.dev/12-wgsl-fundamentals-data-types-and-variables\"><strong><em>1.2 - WGSL Fundamentals - Data Types &amp; Variables</em></strong></a></p><hr>\n<h2 id=\"quick-reference\">Quick Reference</h2>\n<p>A summary of the core concepts for quick lookup.</p><h3 id=\"the-pipeline-stages--their-jobs\">The Pipeline Stages &amp; Their Jobs</h3>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Analogy</th>\n<th>Role (What it does)</th>\n<th>Programmable?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1. Application (CPU)</strong></td>\n<td>The Director</td>\n<td>Prepares all scene data: meshes, materials, transforms, camera info.</td>\n<td>Yes (Rust)</td>\n</tr>\n<tr>\n<td><strong>2. Vertex Shader</strong></td>\n<td>The Layout Artist</td>\n<td>Runs <strong>per-vertex</strong> to calculate its final on-screen position (in Clip Space).</td>\n<td><strong>Yes (WGSL)</strong></td>\n</tr>\n<tr>\n<td><strong>3. Rasterizer</strong></td>\n<td>The Render Farm</td>\n<td><strong>Automatic hardware</strong> step that turns 3D triangles into a 2D grid of fragments.</td>\n<td>No</td>\n</tr>\n<tr>\n<td><strong>4. Fragment Shader</strong></td>\n<td>The Coloring Artist</td>\n<td>Runs <strong>per-fragment</strong> to calculate its final RGBA color.</td>\n<td><strong>Yes (WGSL)</strong></td>\n</tr>\n<tr>\n<td><strong>5. Output Merger</strong></td>\n<td>The Final Print</td>\n<td><strong>Automatic hardware</strong> step that performs depth tests and writes the final color.</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"the-coordinate-space-journey-1\">The Coordinate Space Journey</h3>\n<p>This is the required path a single vertex position takes through the Vertex Shader.</p><pre><code class=\"language-plaintext\">Local Space → World Space → View Space → Clip Space\n</code></pre>\n<p>(The GPU then automatically handles the final conversion to Screen Space)</p><h3 id=\"core-concepts\">Core Concepts</h3>\n<ul>\n<li><strong>Parallelism</strong>: The GPU’s core strength. It processes thousands of vertices and millions of fragments simultaneously. Your shader code is the instruction set for this parallel work.</li>\n<li><strong>Interpolation</strong>: The process by which the <strong>Rasterizer</strong> automatically and smoothly blends data (like colors, UVs, or normals) that was output by the <strong>Vertex Shader</strong> at the corners of a triangle. This provides a unique value for that data to the <strong>Fragment Shader</strong> for every pixel the triangle covers.</li>\n</ul>\n",
            "image": "https://xav.github.io/media/posts/3/cover.png",
            "author": {
                "name": "Xavier Basty Kjellberg"
            },
            "tags": [
                   "wgsl",
                   "shaders",
                   "rust",
                   "bevy"
            ],
            "date_published": "2025-12-09T22:42:59+01:00",
            "date_modified": "2025-12-09T22:42:59+01:00"
        },
        {
            "id": "https://xav.github.io/learning-wgsl-shaders-with-bevy-016-a-practical-journey.html",
            "url": "https://xav.github.io/learning-wgsl-shaders-with-bevy-016-a-practical-journey.html",
            "title": "Learning WGSL Shaders with Bevy 0.16: A Practical Journey",
            "summary": "Does This Sound Familiar? You’re a Bevy developer. You love Rust’s power and safety, and you’ve started to feel comfortable building scenes and game logic. But now you want to create something truly unique - a shimmering shield, a stylized water surface, a custom lighting&hellip;",
            "content_html": "<h2 id=\"does-this-sound-familiar\">Does This Sound Familiar?</h2>\n<p>You’re a Bevy developer. You love Rust’s power and safety, and you’ve started to feel comfortable building scenes and game logic. But now you want to create something truly unique - a shimmering shield, a stylized water surface, a custom lighting model - and you realize you need to write a shader.</p><p>So you open a <code>.wgsl</code> file… and you hit a wall.</p><p>You look for tutorials, but they’re almost all for GLSL or Unity, using a different language and a different engine. You look at the official Bevy examples, but they seem to assume you already know how shaders work. You’re stuck in a frustrating gap: you know the engine, you know the language, but you don’t know how to bridge the two to speak directly to the GPU.</p><p><strong>I created this series because I was that developer.</strong></p><p>This is not a theoretical textbook or a high-level overview. It is the practical, step-by-step guide I wish I had when I was starting. It’s the result of systematically navigating that gap, figuring out the fundamentals, hitting the common pitfalls, and documenting what finally made the concepts “click.”</p><p>My goal is to provide a clear, linear path that takes you from the absolute basics of the graphics pipeline all the way to advanced techniques, with every single concept explained and demonstrated inside a working Bevy project. This is the journey of learning how to think in shaders, and I’m thrilled to share it with you.</p><h2 id=\"our-approach-a-practical-incremental-journey\">Our Approach: A Practical, Incremental Journey</h2>\n<p>This is a <strong>practical, hands-on series</strong>. We will learn by building, not just by reading. Every single article is built around a working Bevy project that you can run, modify, and experiment with. Our philosophy is that you don’t truly understand a concept until you’ve seen it work, broken it, and fixed it again.</p><p>Our journey is structured to build your knowledge from the ground up, ensuring you have a solid foundation before moving on to more complex topics. Here’s the path we’ll take:</p><ol>\n<li><p><strong>First, we’ll build the foundation.</strong> We will demystify the <strong>Graphics Pipeline</strong> and learn the fundamental “alphabet” of WGSL - its data types, variables, and functions. You’ll understand where your code runs and the language it speaks.</p></li>\n<li><p><strong>Then, we’ll take control of geometry.</strong> We’ll dive deep into the <strong>Vertex Shader</strong>, where you will learn to manipulate the very shape of your 3D models. You’ll create waving flags, rippling water, and fields of animated grass, learning how to breathe life into static meshes.</p></li>\n<li><p><strong>Next, we’ll learn to paint those shapes with light and color.</strong> We’ll master the <strong>Fragment Shader</strong>, moving beyond simple colors to create procedural patterns, sample textures, and implement our own lighting models. This is where you’ll define the unique visual identity of your projects.</p></li>\n<li><p><strong>Finally, we’ll explore the advanced frontier.</strong> With the fundamentals in place, we’ll unlock the true power of the GPU, exploring post-processing effects, compute shaders, performance optimization, and how to achieve specific artistic styles.</p></li>\n</ol>\n<p>By the end of this series, you won’t just know how to copy and paste shader code - you will have the confidence and the deep understanding to create your own custom rendering effects from scratch.</p><h2 id=\"who-this-series-is-for-and-what-youll-need\">Who This Series Is For (And What You’ll Need)</h2>\n<p>This series is designed for the curious Bevy developer who is ready to take the next step in their creative journey. If you’re comfortable with the basics of Rust and Bevy but feel like the GPU is still a “black box,” you’re in the perfect place.</p><h3 id=\"the-prerequisites\">The Prerequisites</h3>\n<p>This is not a “from zero” programming course. We’ll be moving at a steady pace, and I’ll assume you have a solid footing in the following areas:</p><ul>\n<li><p><strong>A Good Grasp of Rust:</strong> You don’t need to be a systems-level expert, but you should be comfortable with core concepts like structs, traits, ownership, and the module system.</p></li>\n<li><p><strong>Bevy Fundamentals:</strong> You should have worked through the official Bevy book or built a small project. You know what Components, Systems, and Resources are, and you feel comfortable setting up a basic scene.</p></li>\n<li><p><strong>Basic Vector Math Intuition:</strong> You don’t need a math degree, but you should know what a vector is (<code>Vec3</code>) and have a general idea of what operations like adding, subtracting, or normalizing them mean. We’ll review the more complex math (like dot products and matrices) as we need it, focusing on intuition over raw proofs.</p></li>\n</ul>\n<h3 id=\"what-you-dont-need\">What You Don’t Need</h3>\n<p>This is just as important. You <strong>do not</strong> need:</p><ul>\n<li>Any prior shader programming experience (that’s what we’re here to learn!).</li>\n<li>A deep background in low-level graphics APIs like OpenGL, Vulkan, or DirectX.</li>\n<li>To be a math wizard. A willingness to engage with the concepts is far more important than a formal education in linear algebra.</li>\n</ul>\n<p>Our goal is to build up your knowledge from first principles, right here in the Bevy ecosystem.</p><h2 id=\"setting-up-your-development-playground\">Setting Up Your Development Playground</h2>\n<p>A fast, smooth iteration loop is the secret to enjoying shader development. You want to be able to make a small change to your shader code and see the result instantly, without fighting with long compile times. Let’s create a Bevy project specifically configured for this rapid, creative workflow.</p><h3 id=\"recommended-tools\">Recommended Tools</h3>\n<p>While you can use any text editor, this series is written with a specific, highly effective setup in mind:</p><ul>\n<li><ul>\n<li><strong>Editor</strong>: <strong>Visual Studio Code (VS Code)</strong> is strongly recommended. Its rust-analyzer extension provides top-tier support for Rust, and there are excellent extensions that add syntax highlighting for WGSL, making your shader code much easier to read.</li>\n</ul>\n</li>\n<li><p><strong>Graphics Debugger (Optional but Powerful)</strong>: For advanced debugging, a dedicated tool that can capture and inspect a single frame is invaluable. We won’t need this for the early articles, but knowing it exists is important.</p><ul>\n<li><strong>Windows / Linux</strong>: <strong>RenderDoc</strong> is the open-source industry standard.</li>\n<li><strong>macOS</strong>: Apple’s <strong>Metal GPU Debugger and Frame Capture</strong>, which is built directly into <strong>Xcode</strong>, is the essential tool for debugging Metal applications.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"step-1-install-the-essentials\">Step 1: Install the Essentials</h3>\n<p>First, ensure you have the Rust toolchain installed. If you’re already a Rust developer, you can likely skip this.</p><pre><code class=\"language-bash\"># Install Rust if you don&#39;t have it\ncurl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre>\n<h3 id=\"step-2-create-the-project-and-configure-cargotoml\">Step 2: Create the Project and Configure <code>Cargo.toml</code></h3>\n<p>Let’s create a new Bevy project that will be our home for this entire series.</p><pre><code class=\"language-bash\">mkdir bevy-shader-journey\ncd bevy-shader-journey\ncargo init\n</code></pre>\n<p>Now, replace the contents of your <code>Cargo.toml</code> file with the following configuration. We’re adding a few key dependencies and settings to optimize our development experience.</p><pre><code class=\"language-toml\">[package]\nname = &quot;bevy-shader-journey&quot;\nversion = &quot;0.1.0&quot;\nedition = &quot;2021&quot; # Sticking with the 2021 edition for maximum compatibility\n\n[dependencies]\n# The core Bevy engine, version 0.16\nbevy = { version = &quot;0.16&quot;, features = [&quot;file_watcher&quot;] }\n# For faster compile times in debug builds, you can enable this feature.\n# It doesn&#39;t work on Windows though :(\n# bevy = { version = &quot;0.16&quot;, features = [&quot;dynamic_linking&quot;, &quot;file_watcher&quot;] }\n\n# Interactive demo selection - makes it easy to jump between examples\ninquire = &quot;0.7&quot;\n\n# An incredibly useful debugging tool we&#39;ll use in later articles\n# to tweak shader values in real-time.\nbevy-inspector-egui = &quot;0.23&quot;\n\n# This section makes our local code compile faster (opt-level = 1)\n# while keeping our dependencies fully optimized (opt-level = 3).\n# It&#39;s a great trick for improving Bevy&#39;s debug build times.\n[profile.dev]\nopt-level = 1\n\n[profile.dev.package.&quot;*&quot;]\nopt-level = 3\n</code></pre>\n<h3 id=\"step-3-create-the-project-directory-structure\">Step 3: Create the Project Directory Structure</h3>\n<p>A good project organization will make life much easier as we add more examples. Create the necessary directories and files with this command:</p><pre><code class=\"language-bash\">mkdir -p assets/shaders src/demos src/materials\n\n# Create the module files\ntouch src/demos/mod.rs\ntouch src/materials/mod.rs\n</code></pre>\n<p>This gives us:</p><ul>\n<li><code>assets/shaders/</code>: Where all our <code>.wgsl</code> shader files will live.</li>\n<li><code>src/demos/</code>: Where each article’s interactive example will go.</li>\n<li><code>src/materials/</code>: Where the Rust “glue” code for our custom materials will go.</li>\n</ul>\n<h3 id=\"step-4-set-up-the-demo-selection-system\">Step 4: Set Up the Demo Selection System</h3>\n<p>Here’s where things get interesting. Instead of creating separate example files or constantly commenting and uncommenting code, we’re going to build a smart demo selection system. This will let you easily run any example from the series with a simple command.</p><p>Replace the contents of <code>src/main.rs</code> with this code:</p><pre><code class=\"language-rust\">mod demos;\nmod materials;\n\nuse std::env;\n\nstruct Demo {\n    number: &amp;&#39;static str,\n    title: &amp;&#39;static str,\n    run: fn(),\n}\n\nimpl Demo {\n    fn matches(&amp;self, query: &amp;str) -&gt; bool {\n        let query_lower = query.to_lowercase();\n        self.number.contains(&amp;query_lower) || self.title.to_lowercase().contains(&amp;query_lower)\n    }\n\n    fn display(&amp;self) -&gt; String {\n        format!(&quot;{} - {}&quot;, self.number, self.title)\n    }\n}\n\nfn main() {\n    // Registry of all available demos\n    let demos = vec![\n        Demo {\n            number: &quot;0.0&quot;,\n            title: &quot;Basic Scene Setup&quot;,\n            run: demos::basic_scene::run,\n        },\n        // More demos will be added as we progress through the series\n    ];\n\n    let args: Vec&lt;String&gt; = env::args().skip(1).collect();\n\n    let demo = if args.is_empty() {\n        select_demo_interactive(&amp;demos)\n    } else {\n        let query = args.join(&quot; &quot;);\n        find_and_select_demo(&amp;demos, &amp;query)\n    };\n\n    match demo {\n        Some(d) =&gt; {\n            println!(&quot;\\n🚀 Running: {}\\n&quot;, d.display());\n            (d.run)();\n        }\n        None =&gt; {\n            println!(&quot;No demo selected. Exiting.&quot;);\n            std::process::exit(0);\n        }\n    }\n}\n\nfn find_and_select_demo&lt;&#39;a&gt;(demos: &amp;&#39;a [Demo], query: &amp;str) -&gt; Option&lt;&amp;&#39;a Demo&gt; {\n    let matches: Vec&lt;&amp;Demo&gt; = demos.iter().filter(|d| d.matches(query)).collect();\n\n    match matches.len() {\n        0 =&gt; {\n            eprintln!(&quot;❌ No demos match &#39;{}&#39;\\n&quot;, query);\n            show_available_demos(demos);\n            println!();\n            select_demo_interactive(demos)\n        }\n        1 =&gt; Some(matches[0]),\n        _ =&gt; {\n            eprintln!(&quot;⚠️  Multiple demos match &#39;{}&#39;:\\n&quot;, query);\n            select_from_list(&amp;matches)\n        }\n    }\n}\n\nfn select_demo_interactive&lt;&#39;a&gt;(demos: &amp;&#39;a [Demo]) -&gt; Option&lt;&amp;&#39;a Demo&gt; {\n    println!(&quot;Available demos:&quot;);\n    show_available_demos(demos);\n    println!();\n\n    let demo_refs: Vec&lt;&amp;Demo&gt; = demos.iter().collect();\n    select_from_list(&amp;demo_refs)\n}\n\nfn show_available_demos(demos: &amp;[Demo]) {\n    for demo in demos {\n        println!(&quot;  • {}&quot;, demo.display());\n    }\n}\n\nfn select_from_list&lt;&#39;a&gt;(demos: &amp;[&amp;&#39;a Demo]) -&gt; Option&lt;&amp;&#39;a Demo&gt; {\n    use inquire::Select;\n\n    let options: Vec&lt;String&gt; = demos.iter().map(|d| d.display()).collect();\n    let selected = Select::new(&quot;Select a demo to run:&quot;, options).prompt();\n\n    match selected {\n        Ok(selected_text) =&gt; demos.iter().find(|d| d.display() == selected_text).copied(),\n        Err(_) =&gt; {\n            println!(&quot;Selection cancelled.&quot;);\n            None\n        }\n    }\n}\n</code></pre>\n<p><strong>What does this do?</strong> This system lets you run any demo from the series with intuitive commands:</p><pre><code class=\"language-rust\"># Run by article number\ncargo run 1.1\n\n# Run by partial title match\ncargo run pipeline\n\n# Multiple matches? You&#39;ll get an interactive menu with arrow keys\ncargo run shader\n\n# No arguments? See all available demos and select with arrow keys\ncargo run\n</code></pre>\n<p>As you progress through the series, you’ll simply add new demos to the <code>demos</code> vector in <code>main.rs</code>, and they’ll automatically be available through this selection system. No managing multiple example files or hunting through code!</p><h3 id=\"step-5-create-your-first-demo-module\">Step 5: Create Your First Demo Module</h3>\n<p>Now let’s create the first demo that we registered in our system. Create <code>src/demos/basic_scene.rs</code>:</p><pre><code class=\"language-rust\">use bevy::prelude::*;\n\npub fn run() {\n    App::new()\n        .add_plugins(DefaultPlugins.set(AssetPlugin {\n            // Enable hot reloading\n            watch_for_changes_override: Some(true),\n            ..default()\n        }))\n        .add_systems(Startup, setup)\n        .add_systems(Update, rotate_camera)\n        .run();\n}\n\nfn setup(\n    mut commands: Commands,\n    mut meshes: ResMut&lt;Assets&lt;Mesh&gt;&gt;,\n    mut materials: ResMut&lt;Assets&lt;StandardMaterial&gt;&gt;,\n) {\n    // Add a simple scene we can apply our shaders to\n    commands.spawn((\n        Mesh3d(meshes.add(Cuboid::new(2.0, 2.0, 2.0))),\n        MeshMaterial3d(materials.add(Color::srgb(0.8, 0.7, 0.6))),\n    ));\n\n    // Light\n    commands.spawn((\n        PointLight {\n            shadows_enabled: true,\n            ..default()\n        },\n        Transform::from_xyz(4.0, 8.0, 4.0),\n    ));\n\n    // Camera\n    commands.spawn((\n        Camera3d::default(),\n        Transform::from_xyz(-2.5, 4.5, 9.0).looking_at(Vec3::ZERO, Vec3::Y),\n    ));\n}\n\n// Simple camera rotation to see our shaders from different angles\nfn rotate_camera(time: Res&lt;Time&gt;, mut camera_query: Query&lt;&amp;mut Transform, With&lt;Camera3d&gt;&gt;) {\n    for mut transform in camera_query.iter_mut() {\n        let radius = 9.0;\n        let angle = time.elapsed_secs() * 0.5;\n        transform.translation.x = angle.cos() * radius;\n        transform.translation.z = angle.sin() * radius;\n        transform.look_at(Vec3::ZERO, Vec3::Y);\n    }\n}\n</code></pre>\n<p>And register it in <code>src/demos/mod.rs</code>:</p><pre><code class=\"language-rust\">pub mod basic_scene;\n</code></pre>\n<p><strong>The pattern you’ll follow for every article:</strong></p><ol>\n<li>Create a new module in <code>src/demos/</code> (e.g., <code>first_shader.rs</code>)</li>\n<li>Implement a <code>pub fn run()</code> function that starts a Bevy app</li>\n<li>Add the module to <code>src/demos/mod.rs</code></li>\n<li>Register it in the <code>demos</code> vector in <code>main.rs</code></li>\n</ol>\n<p>That’s it! The demo selection system handles the rest.</p><h3 id=\"step-5-test-your-setup\">Step 5: Test Your Setup</h3>\n<p>You’re all set! Let’s make sure everything works:</p><p>bash</p><pre><code class=\"language-bash\"># Run the basic scene demo directly\ncargo run 0.0\n\n# Or try the interactive selection\ncargo run\n</code></pre>\n<p>You should see a simple scene with a rotating camera orbiting around a cube.</p><p><strong>Try experimenting with the selection system:</strong></p><pre><code class=\"language-bash\">cargo run basic     # Matches &quot;Basic Scene Setup&quot;\ncargo run scene     # Same result\ncargo run xyz       # No match - shows all demos and lets you select\n</code></pre>\n<p><strong>Congratulations!</strong> You now have a robust development environment with:</p><ul>\n<li>Fast iteration with hot-reloading for shader changes</li>\n<li>An intuitive demo selection system for jumping between examples</li>\n<li>A clean project structure that will scale as you learn</li>\n</ul>\n<p>When you save changes to a shader file in the <code>assets</code> directory, Bevy will automatically detect the change and reload it in real-time. This instant feedback is the key to learning and experimenting with shaders effectively.</p><h2 id=\"understanding-the-project-structure\">Understanding the Project Structure</h2>\n<p>Here’s how everything fits together:</p><pre><code class=\"language-plaintext\">bevy-shader-journey/\n├── assets/\n│   └── shaders/              # Your .wgsl shader files\n│       └── first_shader.wgsl # (We&#39;ll create these as we go)\n├── src/\n│   ├── main.rs               # Demo selection system\n│   ├── demos/                # One module per article\n│   │   ├── mod.rs\n│   │   ├── basic_scene.rs    # Article 0.0\n│   │   └── first_shader.rs   # (Article 1.1, etc.)\n│   └── materials/            # Custom material definitions\n│       └── mod.rs\n└── Cargo.toml\n</code></pre>\n<p><strong>The workflow:</strong></p><ol>\n<li>Read an article to understand the concepts</li>\n<li>Run the demo: <code>cargo run [article-number]</code></li>\n<li>Look at the code in <code>src/demos/[name].rs</code></li>\n<li>Examine the shader in <code>assets/shaders/[name].wgsl</code></li>\n<li>Experiment - change values, break things, see what happens!</li>\n<li>Move to the next article when ready</li>\n</ol>\n<h2 id=\"a-mindset-for-success\">A Mindset for Success</h2>\n<p>You are now ready to begin. As you dive into the world of shaders, keeping a few practical tips in mind will make the learning process smoother and more enjoyable.</p><ul>\n<li><strong>Start Simple, Then Iterate.</strong> The golden rule of shader development. Always begin with the absolute most basic version of an effect that works, even if it’s just a solid color. Once you have a working baseline, add complexity one small step at a time. This makes debugging infinitely easier.</li>\n<li><strong>Embrace the Visual Debugging Loop.</strong> Your primary debugging tool is the screen itself. Is the effect too bright? Multiply by 0.5. Is it upside down? Multiply a coordinate by <code>-1.0</code>. Get comfortable making small, incremental changes and immediately observing the visual result.</li>\n<li><strong>Use Descriptive Variable Names.</strong> Shaders can quickly become a maze of vector math. A variable named <code>surfaceToLightDirection</code> is a thousand times clearer than <code>vecL</code>. Your future self, trying to debug the code, will thank you.</li>\n<li><strong>Save Your Progress Frequently.</strong> Shader development is highly experimental. You’ll often go down a path that leads to a visual mess. Using a version control system like Git to commit frequently after each small success will give you the freedom to experiment without fear of losing your working code.</li>\n<li><strong>Jump Between Examples Freely.</strong> One of the beauties of our demo selection system is that you can easily revisit previous techniques. Forgot how normal mapping works? Just <code>cargo run normal</code> and refresh your memory. Want to compare two approaches? Run them side by side!</li>\n</ul>\n<h2 id=\"lets-get-started\">Let’s Get Started!</h2>\n<p>Your development environment is configured, your mindset is right, and the path is laid out before you. That blank <code>.wgsl</code> file is no longer an intimidating obstacle - it’s a canvas waiting for your creativity.</p><p>The demo selection system gives you the freedom to explore at your own pace, jumping between concepts and building your understanding incrementally. Each article in this series adds a new demo to your toolkit, and by the end, you’ll have a comprehensive library of shader techniques at your fingertips.</p><p>It’s time to dive in and learn how to speak directly to the GPU.</p><p>Next up: [1.1 - Understanding the Graphics Pipeline]</p><hr>\n<h2 id=\"quick-reference\">Quick Reference</h2>\n<p><strong>Running Demos:</strong></p><pre><code class=\"language-bash\">cargo run 1.1              # By article number\ncargo run pipeline         # By keyword\ncargo run                  # Interactive selection\n</code></pre>\n<p><strong>Project Structure:</strong></p><ul>\n<li><code>src/main.rs</code> - Demo selection system</li>\n<li><code>src/demos/</code> - One module per article (each exports <code>pub fn run()</code>)</li>\n<li><code>src/materials/</code> - Custom material definitions</li>\n<li><code>assets/shaders/</code> - WGSL shader files</li>\n</ul>\n<p><strong>Development Tips:</strong></p><ul>\n<li>Save a shader file → Bevy auto-reloads it</li>\n<li>Small changes → Immediate visual feedback</li>\n<li>Commit working code frequently</li>\n<li>Use descriptive names in shader code</li>\n</ul>\n",
            "image": "https://xav.github.io/media/posts/2/cover.png",
            "author": {
                "name": "Xavier Basty Kjellberg"
            },
            "tags": [
                   "wgsl",
                   "shaders",
                   "rust",
                   "bevy"
            ],
            "date_published": "2025-08-29T22:38:00+02:00",
            "date_modified": "2025-12-09T22:39:17+01:00"
        }
    ]
}
